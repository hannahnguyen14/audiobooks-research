{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import mannwhitneyu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         asin                                           document  label\n",
      "0  B01I5J99HW  pinching his bearded cheek his brown hair was ...      1\n",
      "1  B012DWPABG  the blare of a car horn drag jake from a sound...      1\n",
      "2  B002V1C1NK  screens and keyboards though that may sound di...      0\n",
      "3  B07LF84VVR  what in the name of hell is on your head arriv...      1\n",
      "4  0008322651  it's meals on silver trays we went there to me...      0\n"
     ]
    }
   ],
   "source": [
    "corpus_df = pd.read_csv('../data/corpus/Romance.csv')\n",
    "print(corpus_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Sum Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract documents and labels from the DataFrame\n",
    "corpus = corpus_df['document']\n",
    "labels = corpus_df['label']\n",
    "\n",
    "# Calculate TF-IDF \n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, max_df=0.5, min_df=10, stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "terms_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create DataFrame for TF-IDF scores with labels\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=terms_tfidf)\n",
    "tfidf_df['Genre'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>action</th>\n",
       "      <th>actual</th>\n",
       "      <th>actually</th>\n",
       "      <th>added</th>\n",
       "      <th>admit</th>\n",
       "      <th>afraid</th>\n",
       "      <th>age</th>\n",
       "      <th>ago</th>\n",
       "      <th>ahead</th>\n",
       "      <th>...</th>\n",
       "      <th>wouldn</th>\n",
       "      <th>writing</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.059977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077706</td>\n",
       "      <td>0.064093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.065002</td>\n",
       "      <td>0.082402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059438</td>\n",
       "      <td>0.048096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 889 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         able    action    actual  actually     added  admit  afraid  age  \\\n",
       "0    0.000000  0.071464  0.000000  0.000000  0.000000    0.0     0.0  0.0   \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.000000    0.0     0.0  0.0   \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000    0.0     0.0  0.0   \n",
       "3    0.000000  0.000000  0.000000  0.000000  0.071325    0.0     0.0  0.0   \n",
       "4    0.059977  0.000000  0.077706  0.064093  0.000000    0.0     0.0  0.0   \n",
       "..        ...       ...       ...       ...       ...    ...     ...  ...   \n",
       "175  0.065002  0.082402  0.000000  0.000000  0.000000    0.0     0.0  0.0   \n",
       "176  0.000000  0.000000  0.000000  0.000000  0.000000    0.0     0.0  0.0   \n",
       "177  0.000000  0.000000  0.000000  0.000000  0.000000    0.0     0.0  0.0   \n",
       "178  0.000000  0.000000  0.000000  0.000000  0.000000    0.0     0.0  0.0   \n",
       "179  0.000000  0.000000  0.000000  0.000000  0.000000    0.0     0.0  0.0   \n",
       "\n",
       "     ago     ahead  ...    wouldn   writing     wrong      yeah      year  \\\n",
       "0    0.0  0.000000  ...  0.000000  0.000000  0.000000  0.051985  0.000000   \n",
       "1    0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2    0.0  0.000000  ...  0.000000  0.086782  0.000000  0.000000  0.050370   \n",
       "3    0.0  0.073977  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4    0.0  0.000000  ...  0.000000  0.000000  0.063202  0.000000  0.000000   \n",
       "..   ...       ...  ...       ...       ...       ...       ...       ...   \n",
       "175  0.0  0.000000  ...  0.055273  0.000000  0.000000  0.000000  0.195525   \n",
       "176  0.0  0.085019  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "177  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "178  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.059438   \n",
       "179  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        years       yes  york     young  Genre  \n",
       "0    0.000000  0.094968   0.0  0.000000      1  \n",
       "1    0.000000  0.000000   0.0  0.000000      1  \n",
       "2    0.000000  0.000000   0.0  0.130739      0  \n",
       "3    0.000000  0.051169   0.0  0.000000      1  \n",
       "4    0.072992  0.000000   0.0  0.000000      0  \n",
       "..        ...       ...   ...       ...    ...  \n",
       "175  0.000000  0.000000   0.0  0.000000      1  \n",
       "176  0.000000  0.058807   0.0  0.000000      0  \n",
       "177  0.000000  0.068926   0.0  0.000000      1  \n",
       "178  0.048096  0.000000   0.0  0.000000      0  \n",
       "179  0.000000  0.000000   0.0  0.000000      0  \n",
       "\n",
       "[180 rows x 889 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign & normalize words rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign ranks based on TF-IDF scores \n",
    "def assign_flipped_ranks(row):\n",
    "    ranks = row.rank(method='min', ascending=False)  # Assign ranks (higher score gets lower rank)\n",
    "    max_rank = ranks.max() \n",
    "    if max_rank > 0:\n",
    "        ranks = (max_rank + 1 - ranks) / max_rank #normalize rank to 0-1 scale -> closer to 1 is better rank\n",
    "    ranks[row == 0] = 0  # Assign rank 0 to words not present in the document\n",
    "    return ranks\n",
    "\n",
    "# Apply rank assignment\n",
    "tfidf_ranks_df = tfidf_df.drop(columns=['Genre']).apply(assign_flipped_ranks, axis=1)\n",
    "tfidf_ranks_df['Genre'] = tfidf_df['Genre']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate words rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate DataFrame by genre\n",
    "sf_ranks = tfidf_ranks_df[tfidf_ranks_df['Genre'] == 1].drop(columns=['Genre'])\n",
    "non_sf_ranks = tfidf_ranks_df[tfidf_ranks_df['Genre'] == 0].drop(columns=['Genre'])\n",
    "\n",
    "# Calculate sum rank for each word in each genre\n",
    "sf_agg_ranks = sf_ranks.sum(axis=0).sort_values(ascending=False)\n",
    "non_sf_agg_ranks = non_sf_ranks.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# DataFrame for aggregated ranks\n",
    "sf_agg_ranks_df = pd.DataFrame({'Word': sf_agg_ranks.index, 'SF Total Rank': sf_agg_ranks.values})\n",
    "non_sf_agg_ranks_df = pd.DataFrame({'Word': non_sf_agg_ranks.index, 'Non-SF Total Rank': non_sf_agg_ranks.values})\n",
    "\n",
    "agg_ranks_df = pd.merge(sf_agg_ranks_df, non_sf_agg_ranks_df, on='Word', how='outer')\n",
    "agg_ranks_df['Difference'] = agg_ranks_df['SF Total Rank'] - agg_ranks_df['Non-SF Total Rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mann–Whitney U Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the results\n",
    "p_values = []\n",
    "\n",
    "# Perform the Mann–Whitney U Test for each word\n",
    "for word in agg_ranks_df['Word']:\n",
    "    # Extract the rank values for the word from the SF and Non-SF DataFrames\n",
    "    sf_ranks_word = sf_ranks[word].values\n",
    "    non_sf_ranks_word = non_sf_ranks[word].values\n",
    "\n",
    "    # Perform the Mann–Whitney U Test\n",
    "    stat, p_value = mannwhitneyu(sf_ranks_word, non_sf_ranks_word, alternative='two-sided')\n",
    "    p_values.append(p_value)\n",
    "\n",
    "# Append result\n",
    "agg_ranks_df['P-Value'] = p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>SF Total Rank</th>\n",
       "      <th>Non-SF Total Rank</th>\n",
       "      <th>Difference</th>\n",
       "      <th>P-Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eyes</td>\n",
       "      <td>33.959137</td>\n",
       "      <td>8.367844</td>\n",
       "      <td>25.591293</td>\n",
       "      <td>9.304057e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wasn</td>\n",
       "      <td>23.735778</td>\n",
       "      <td>6.481327</td>\n",
       "      <td>17.254451</td>\n",
       "      <td>1.015394e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hand</td>\n",
       "      <td>20.608794</td>\n",
       "      <td>5.714857</td>\n",
       "      <td>14.893937</td>\n",
       "      <td>1.654684e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>face</td>\n",
       "      <td>22.536238</td>\n",
       "      <td>9.031123</td>\n",
       "      <td>13.505115</td>\n",
       "      <td>1.351366e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heart</td>\n",
       "      <td>15.429550</td>\n",
       "      <td>2.806908</td>\n",
       "      <td>12.622642</td>\n",
       "      <td>3.003597e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>let</td>\n",
       "      <td>12.524833</td>\n",
       "      <td>7.446021</td>\n",
       "      <td>5.078812</td>\n",
       "      <td>3.388412e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>gone</td>\n",
       "      <td>8.088036</td>\n",
       "      <td>3.078665</td>\n",
       "      <td>5.009371</td>\n",
       "      <td>3.279152e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>voice</td>\n",
       "      <td>11.099826</td>\n",
       "      <td>6.716390</td>\n",
       "      <td>4.383436</td>\n",
       "      <td>2.900038e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>ago</td>\n",
       "      <td>8.364998</td>\n",
       "      <td>5.163137</td>\n",
       "      <td>3.201861</td>\n",
       "      <td>7.203511e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>yes</td>\n",
       "      <td>7.678784</td>\n",
       "      <td>4.873559</td>\n",
       "      <td>2.805225</td>\n",
       "      <td>1.283229e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  SF Total Rank  Non-SF Total Rank  Difference       P-Value\n",
       "0    eyes      33.959137           8.367844   25.591293  9.304057e-11\n",
       "1    wasn      23.735778           6.481327   17.254451  1.015394e-06\n",
       "2    hand      20.608794           5.714857   14.893937  1.654684e-06\n",
       "3    face      22.536238           9.031123   13.505115  1.351366e-03\n",
       "4   heart      15.429550           2.806908   12.622642  3.003597e-05\n",
       "..    ...            ...                ...         ...           ...\n",
       "76    let      12.524833           7.446021    5.078812  3.388412e-03\n",
       "77   gone       8.088036           3.078665    5.009371  3.279152e-03\n",
       "78  voice      11.099826           6.716390    4.383436  2.900038e-03\n",
       "79    ago       8.364998           5.163137    3.201861  7.203511e-03\n",
       "80    yes       7.678784           4.873559    2.805225  1.283229e-03\n",
       "\n",
       "[81 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_list = agg_ranks_df[(agg_ranks_df['Difference'] > 2) & (agg_ranks_df['P-Value'] < 0.009)].sort_values(by='Difference', ascending=False).reset_index(drop=True)\n",
    "filter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.431518026277569"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_list['Difference'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping documents with all-zero vectorization (no matching terms in top list): [114 151]\n",
      "Training set size: 124\n",
      "Test set size: 54\n",
      "Class distribution in training set: label\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Class distribution in test set: label\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "top_terms = filter_list['Word'].values\n",
    "\n",
    "# Vectorize the original corpus\n",
    "vectorizer = TfidfVectorizer(vocabulary=top_terms)  # Restrict vectorization to top terms\n",
    "selected_features = vectorizer.fit_transform(corpus_df['document'])  # Apply vectorizer to original documents\n",
    "\n",
    "\n",
    "# Call out documents with all zero\n",
    "zero_vector_docs = np.where(selected_features.toarray().sum(axis=1) == 0)[0]  # Find indices of documents with all-zero vectors\n",
    "\n",
    "# Drop documents with all-zero vectorization\n",
    "if len(zero_vector_docs) > 0:\n",
    "    print(f\"Dropping documents with all-zero vectorization (no matching terms in top list): {zero_vector_docs}\")\n",
    "    select_corpus_df = corpus_df.drop(index=zero_vector_docs).reset_index(drop=True)  # Drop and reset index\n",
    "    selected_features = vectorizer.fit_transform(select_corpus_df['document'])  # Re-vectorize without dropped docs\n",
    "else:\n",
    "    print(\"No documents with all-zero vectorization found.\")\n",
    "\n",
    "# Create DataFrame with selected features (top terms)\n",
    "selected_df = pd.DataFrame(selected_features.toarray(), columns=top_terms)\n",
    "\n",
    "# Add genre labels \n",
    "selected_df['label'] = select_corpus_df['label']\n",
    "\n",
    "# Prepare X and y for training\n",
    "X = selected_df.drop(columns='label')\n",
    "y = selected_df['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=55)\n",
    "\n",
    "# Check the balanced split\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])\n",
    "print(\"Class distribution in training set:\", y_train.value_counts(normalize=True))\n",
    "print(\"Class distribution in test set:\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "      Non-Science Fiction       0.77      0.89      0.83        27\n",
      "Science Fiction & Fantasy       0.87      0.74      0.80        27\n",
      "\n",
      "                 accuracy                           0.81        54\n",
      "                macro avg       0.82      0.81      0.81        54\n",
      "             weighted avg       0.82      0.81      0.81        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000, C = 3)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Non-Science Fiction\", \"Science Fiction & Fantasy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.45494394726628934\n",
      "Adjusted Threshold Test Set Performance:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "      Non-Science Fiction       0.86      0.89      0.87        27\n",
      "Science Fiction & Fantasy       0.88      0.85      0.87        27\n",
      "\n",
      "                 accuracy                           0.87        54\n",
      "                macro avg       0.87      0.87      0.87        54\n",
      "             weighted avg       0.87      0.87      0.87        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the logistic regression model using the training set\n",
    "final_model = LogisticRegression(max_iter=1000, random_state=42, C = 3)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_probs = final_model.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "# Calculate precision-recall pairs for different thresholds\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "# Choose optimized threshold \n",
    "best_threshold_index = np.argmax(2 * (precisions * recalls) / (precisions + recalls))\n",
    "best_threshold = thresholds[best_threshold_index]\n",
    "print(\"Best threshold:\", best_threshold)\n",
    "\n",
    "# Make predictions using the new threshold\n",
    "y_pred_adj = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluate performance using the adjusted threshold\n",
    "print(\"Adjusted Threshold Test Set Performance:\")\n",
    "print(classification_report(y_test, y_pred_adj, target_names=[\"Non-Science Fiction\", \"Science Fiction & Fantasy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Predictive Terms for Science Fiction & Fantasy:\n",
      "          Term  Coefficient\n",
      "0         eyes     2.219343\n",
      "1         hell     2.197138\n",
      "2         love     2.174293\n",
      "3      laughed     1.763326\n",
      "4         arms     1.466400\n",
      "5        shook     1.418833\n",
      "6         wasn     1.396065\n",
      "7        tears     1.310844\n",
      "8      sitting     1.306171\n",
      "9   expression     1.296119\n",
      "10        tell     1.269924\n",
      "11      opened     1.268675\n",
      "12         hey     1.249319\n",
      "13        kept     1.245219\n",
      "14         eye     1.239224\n",
      "15      tongue     1.210864\n",
      "16        kiss     1.194850\n",
      "17        yeah     1.179498\n",
      "18       happy     1.174306\n",
      "19          oh     1.150359\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature importance (coefficients)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Term': X.columns,\n",
    "    'Coefficient': logreg.coef_[0]\n",
    "})\n",
    "\n",
    "feature_importance = feature_importance.sort_values(by='Coefficient', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Top Predictive Terms for Science Fiction & Fantasy:\")\n",
    "print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reserved test set\n",
    "reserved_df = pd.read_csv('../data/corpus/reserved_test_pred.csv')\n",
    "\n",
    "original_reserved_df = reserved_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1 documents with all-zero vectorization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the reserved test set\n",
    "reserved_features = vectorizer.transform(reserved_df['document'])  # Apply trained vectorizer\n",
    "\n",
    "# Cull out documents with all-zero vectorization\n",
    "zero_vector_docs = np.where(reserved_features.toarray().sum(axis=1) == 0)[0]  \n",
    "\n",
    "# Store ASINs of removed documents \n",
    "removed_asins = reserved_df.iloc[zero_vector_docs]['asin'].tolist()\n",
    "\n",
    "# Drop documents with all-zero vectorization\n",
    "if len(zero_vector_docs) > 0:\n",
    "    print(f\"Removing {len(zero_vector_docs)} documents with all-zero vectorization.\")\n",
    "    reserved_df = reserved_df.drop(index=zero_vector_docs).reset_index(drop=True)  # Drop and reset index\n",
    "    reserved_features = vectorizer.transform(reserved_df['document'])  # Re-vectorize remaining documents\n",
    "else:\n",
    "    print(\"No documents with all-zero vectorization found.\")\n",
    "\n",
    "#Predict probabilities using the final logistic regression model\n",
    "reserved_probs = final_model.predict_proba(reserved_features)[:, 1]  # Probability for class 1 (main genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the reserved_df\n",
    "main_genre = \"Romance\" \n",
    "reserved_df[f\"{main_genre}_Prob\"] = reserved_probs.round(2)\n",
    "\n",
    "# Reinsert removed documents with placeholder values\n",
    "original_reserved_df[f\"{main_genre}_Prob\"] = None  # Initialize the probability column with None\n",
    "original_reserved_df.loc[\n",
    "    original_reserved_df['asin'].isin(reserved_df['asin']), \n",
    "    f\"{main_genre}_Prob\"\n",
    "] = reserved_df[f\"{main_genre}_Prob\"].values\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "original_reserved_df.to_csv('../data/corpus/reserved_test_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
